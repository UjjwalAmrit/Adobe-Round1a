# -*- coding: utf-8 -*-
"""1A.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N-04j90DpGE8eqNXMn4txRC_roZMQ781
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install faiss-cpu
# %pip install PyMuPDF

import fitz
import json
import os
import re
from collections import Counter
from datetime import datetime
from typing import List, Dict, Tuple
import statistics

class UniversalPDFExtractor:
    def __init__(self, cache_dir):
        self.cache_dir = cache_dir
        self.stop_words = {
            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does',
            'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can',
            'this', 'that', 'these', 'those', 'a', 'an', 'as', 'it', 'its', 'they', 'them',
            'their', 'we', 'our', 'you', 'your', 'he', 'him', 'his', 'she', 'her', 'i',
            'me', 'my', 'not', 'no', 'yes', 'if', 'then', 'than', 'so', 'very', 'just',
            'introduction', 'conclusion', 'overview', 'summary', 'chapter', 'section',
            'page', 'figure', 'table', 'references', 'bibliography', 'appendix'
        }

    def extract_text_with_fonts(self, pdf_path: str) -> Tuple[List[Dict], str]:
        print(f"üìñ Processing: {os.path.basename(pdf_path)}")

        doc = fitz.open(pdf_path)
        raw_blocks = []
        full_document_text = ""

        for page_num, page in enumerate(doc, start=1):
            page_text = page.get_text()
            full_document_text += page_text + " "
            text_dict = page.get_text("dict")
            for block in text_dict["blocks"]:
                if block.get('type') == 0:
                    for line in block["lines"]:
                        merged_text = ""
                        font_sizes = []
                        bold_flags = []
                        italic_flags = []
                        y_positions = []
                        x_positions = []
                        font_names = []

                        for span in line['spans']:
                            merged_text += span['text']
                            font_sizes.append(span['size'])
                            bold_flags.append(bool(span['flags'] & 2 ** 4))
                            italic_flags.append(bool(span['flags'] & 2 ** 1))
                            y_positions.append(float(span['bbox'][1]))
                            x_positions.append(float(span['bbox'][0]))
                            font_names.append(span.get('font', 'Arial'))

                        merged_text = merged_text.strip()
                        if merged_text and len(merged_text) > 1:
                            clean_text = re.sub(r'Page\s*\d+$', '', merged_text).strip()
                            if clean_text:
                                raw_blocks.append({
                                    "text": clean_text,
                                    "font_size": round(max(font_sizes) if font_sizes else 12, 1),
                                    "avg_font_size": round(sum(font_sizes) / len(font_sizes) if font_sizes else 12, 1),
                                    "font_name": font_names[0] if font_names else "Arial",
                                    "is_bold": any(bold_flags),
                                    "is_italic": any(italic_flags),
                                    "all_bold": all(bold_flags) if bold_flags else False,
                                    "page": page_num,
                                    "y": sum(y_positions) / len(y_positions) if y_positions else 0,
                                    "x": sum(x_positions) / len(x_positions) if x_positions else 0,
                                    "y_start": min(y_positions) if y_positions else 0,
                                    "y_end": max(y_positions) if y_positions else 0,
                                    "char_count": len(clean_text),
                                    "word_count": len(clean_text.split())
                                })

        doc.close()
        consolidated_blocks = self.consolidate_text_blocks(raw_blocks)

        print(f"‚úÖ Extracted {len(raw_blocks)} raw blocks ‚Üí {len(consolidated_blocks)} consolidated blocks")
        return consolidated_blocks, full_document_text

    def consolidate_text_blocks(self, raw_blocks: List[Dict]) -> List[Dict]:
        if not raw_blocks:
            return []

        consolidated = []
        i = 0

        while i < len(raw_blocks):
            current_block = raw_blocks[i]
            merged_block = self.try_merge_blocks(raw_blocks, i)
            if merged_block:
                consolidated.append(merged_block['block'])
                i = merged_block['next_index']
            else:
                consolidated.append(current_block)
                i += 1

        return consolidated

    def try_merge_blocks(self, blocks: List[Dict], start_index: int) -> Dict:
        current = blocks[start_index]
        if len(current['text']) > 200:
            return None

        merged_text = current['text']
        end_index = start_index

        for i in range(start_index + 1, min(start_index + 5, len(blocks))):
            next_block = blocks[i]
            if (next_block['page'] != current['page'] or
                abs(next_block['y'] - current['y']) > 50):
                break
            should_merge = self.should_merge_blocks(current, next_block, merged_text)
            if should_merge:
                separator = self.get_merge_separator(merged_text, next_block['text'])
                merged_text += separator + next_block['text']
                end_index = i
                if len(merged_text) > 300:
                    break
            else:
                break
        if end_index > start_index:
            merged_block = current.copy()
            merged_block['text'] = merged_text.strip()
            merged_block['word_count'] = len(merged_text.split())
            merged_block['char_count'] = len(merged_text)
            return {
                'block': merged_block,
                'next_index': end_index + 1
            }
        return None

    def should_merge_blocks(self, current: Dict, next_block: Dict, current_merged_text: str) -> bool:
        if (abs(current['font_size'] - next_block['font_size']) < 0.5 and
            current['is_bold'] == next_block['is_bold']):

            y_distance = abs(next_block['y'] - current['y'])
            if y_distance < 20:
                return True
            if y_distance < 35:
                current_text = current_merged_text.strip()
                next_text = next_block['text'].strip()
                if (current_text.endswith('.') and
                    next_text and next_text[0].isupper() and
                    len(next_text.split()) > 3):
                    return False
                if not current_text.endswith(('.', '!', '?', ':')):
                    return True
                if next_text and next_text[0].islower():
                    return True
                if len(current_text) < 50 and len(next_text) < 50:
                    return True
        return False

    def get_merge_separator(self, current_text: str, next_text: str) -> str:
        current_text = current_text.strip()
        next_text = next_text.strip()
        if current_text.endswith('-'):
            return ''
        if next_text and next_text[0] in '.,;:!?)]}':
            return ''
        if current_text and not current_text.endswith(' '):
            return ' '
        return ''

    def analyze_font_structure(self, text_blocks: List[Dict]) -> Dict:
        print("üîç Analyzing font structure...")
        meaningful_blocks = [
            block for block in text_blocks
            if len(block['text']) > 5 and block['word_count'] >= 2
        ]
        if not meaningful_blocks:
            return {'body_font_size': 12, 'heading_hierarchy': [], 'title_candidates': []}
        font_sizes = [block['font_size'] for block in meaningful_blocks]
        font_size_counts = Counter(font_sizes)
        body_font_size = font_size_counts.most_common(1)[0][0]
        unique_sizes = sorted(set(font_sizes), reverse=True)
        font_stats = {
            'body_font_size': body_font_size,
            'min_size': min(font_sizes),
            'max_size': max(font_sizes),
            'median_size': statistics.median(font_sizes),
            'size_range': max(font_sizes) - min(font_sizes)
        }
        heading_hierarchy = self.create_heading_hierarchy(unique_sizes, body_font_size, font_size_counts)
        title_candidates = self.identify_title_candidates(meaningful_blocks, font_stats, heading_hierarchy)
        print(f"   üìä Body font: {body_font_size}pt")
        print(f"   üìê Font range: {font_stats['min_size']}-{font_stats['max_size']}pt")
        print(f"   üèóÔ∏è Heading levels: {len(heading_hierarchy)}")
        return {
            'body_font_size': body_font_size,
            'font_stats': font_stats,
            'heading_hierarchy': heading_hierarchy,
            'title_candidates': title_candidates,
            'size_counts': font_size_counts
        }

    def create_heading_hierarchy(self, unique_sizes: List[float], body_font_size: float, size_counts: Counter) -> List[Dict]:
        hierarchy = []
        min_heading_diff = 0.5
        potential_heading_sizes = [
            size for size in unique_sizes
            if size > body_font_size + min_heading_diff
        ]
        if not potential_heading_sizes and len(unique_sizes) > 1:
            potential_heading_sizes = unique_sizes[:min(3, len(unique_sizes))]
        heading_levels = ['H1', 'H2', 'H3']
        for i, size in enumerate(potential_heading_sizes[:3]):
            frequency = size_counts[size]
            size_diff = size - body_font_size
            confidence = min(100, (size_diff * 10) + (frequency * 2))
            hierarchy.append({
                'level': heading_levels[i],
                'font_size': size,
                'size_diff': size_diff,
                'frequency': frequency,
                'confidence': confidence
            })
        return hierarchy

    def identify_title_candidates(self, text_blocks: List[Dict], font_stats: Dict, hierarchy: List[Dict]) -> List[Dict]:
        candidates = []
        early_blocks = [b for b in text_blocks if b['page'] <= 3]
        for i, block in enumerate(early_blocks):
            text = block['text'].strip()
            if len(text) < 6 or len(text) > 300:
                continue
            skip_patterns = [
                r'^(page|fig|figure|table)\s*\d+',
                r'^\d+$',
                r'^[a-z]+@[a-z]+\.',
                r'^https?://',
                r'^doi:',
                r'^issn:',
                r'^copyright',
                r'^¬©'
            ]
            if any(re.search(pattern, text.lower()) for pattern in skip_patterns):
                continue
            score = self.calculate_title_score(block, i, font_stats, hierarchy)
            if score > 20:
                candidates.append({
                    'text': text,
                    'score': score,
                    'page': block['page'],
                    'position': i,
                    'font_size': block['font_size'],
                    'is_bold': block['is_bold'],
                    'debug': f"Size:{block['font_size']}, Bold:{block['is_bold']}, Pos:{i}"
                })
        candidates.sort(key=lambda x: x['score'], reverse=True)
        return candidates

    def calculate_title_score(self, block: Dict, position: int, font_stats: Dict, hierarchy: List[Dict]) -> float:
        text = block['text']
        score = 0
        font_size = block['font_size']
        body_size = font_stats['body_font_size']
        max_size = font_stats['max_size']
        if font_size >= max_size:
            score += 40
        elif font_size > body_size:
            size_ratio = (font_size - body_size) / (max_size - body_size) if max_size > body_size else 0
            score += 40 * size_ratio
        position_score = max(0, 30 - position * 2)
        score += position_score
        if block['page'] == 1:
            score += 25
        elif block['page'] == 2:
            score += 15
        elif block['page'] == 3:
            score += 5
        if block['is_bold'] or block['all_bold']:
            score += 20
        word_count = block['word_count']
        if 3 <= word_count <= 15:
            score += 15
        elif 2 <= word_count <= 20:
            score += 10
        elif word_count > 20:
            score += 5
        if text.isupper():
            score += 15
        elif text.istitle():
            score += 10
        if ':' in text:
            score += 10
        academic_keywords = ['policy', 'analysis', 'study', 'research', 'theory', 'framework',
                             'system', 'approach', 'model', 'development', 'management', 'design']
        keyword_matches = sum(1 for keyword in academic_keywords if keyword.lower() in text.lower())
        score += min(10, keyword_matches * 3)
        if block['y'] < 200:
            score += 10
        elif block['y'] < 400:
            score += 5
        if len(text) < 10:
            score -= 5
        elif len(text) > 150:
            score -= 10
        return score

    def detect_title_universal(self, text_blocks: List[Dict], font_analysis: Dict) -> str:
        candidates = font_analysis['title_candidates']
        if not candidates:
            return "Document Title Not Found"
        best_candidate = candidates[0]
        print(f"üéØ Title Detection Results:")
        print(f"   Best: '{best_candidate['text'][:80]}...' (Score: {best_candidate['score']:.1f})")
        print(f"   Font: {best_candidate['font_size']}pt, Bold: {best_candidate['is_bold']}, Page: {best_candidate['page']}")
        if best_candidate['score'] < 30:
            print(f"   ‚ö†Ô∏è Low confidence, trying fallback...")
            first_page_blocks = [b for b in text_blocks if b['page'] == 1]
            if first_page_blocks:
                largest_font_block = max(first_page_blocks, key=lambda x: x['font_size'])
                if len(largest_font_block['text']) > 10:
                    return largest_font_block['text']
        return best_candidate['text']

    def detect_headings_hierarchical(self, text_blocks: List[Dict], font_analysis: Dict) -> List[Dict]:
        print("üîç Detecting headings using hierarchical analysis...")
        hierarchy = font_analysis['heading_hierarchy']
        body_font_size = font_analysis['body_font_size']
        if not hierarchy:
            print("   ‚ö†Ô∏è No clear heading hierarchy found")
            return []
        size_to_level = {}
        for h in hierarchy:
            size_to_level[h['font_size']] = h['level']
        headings = []
        seen_headings = set()
        for block in text_blocks:
            text = block['text'].strip()
            if text.lower() in seen_headings:
                continue
            if not self.is_valid_heading_text(text):
                continue
            font_size = block['font_size']
            heading_level = None
            confidence = 0
            if font_size in size_to_level:
                heading_level = size_to_level[font_size]
                confidence = 80
            else:
                closest_size = min(size_to_level.keys(), key=lambda x: abs(x - font_size))
                if abs(closest_size - font_size) <= 0.5:
                    heading_level = size_to_level[closest_size]
                    confidence = 60
                elif font_size > body_font_size + 1.0:
                    size_diff = font_size - body_font_size
                    if size_diff >= 4:
                        heading_level = 'H1'
                    elif size_diff >= 2:
                        heading_level = 'H2'
                    else:
                        heading_level = 'H3'
                    confidence = 40
            if heading_level:
                confidence += self.calculate_heading_confidence(block, text)
                if confidence >= 60:
                    headings.append({
                        "level": heading_level,
                        "text": text,
                        "page": block['page'],
                        "confidence": confidence,
                        "font_size": font_size
                    })
                    seen_headings.add(text.lower())
        headings.sort(key=lambda x: (x['page'], -x['confidence']))
        final_headings = self.validate_heading_sequence(headings)
        print(f"‚úÖ Detected {len(final_headings)} headings using hierarchical analysis")
        return final_headings

    def is_valid_heading_text(self, text: str) -> bool:
        if len(text) < 3 or len(text) > 200:
            return False
        if not any(c.isalpha() for c in text):
            return False
        reject_patterns = [
            r'^page\s*\d+',
            r'^\d+$',
            r'^[a-z]$',
            r'^(and|the|of|in|to|for|with|by|from|at|on)$',
            r'^(young|old|aged|rats|mice|rest|long|short)$',
            r'^\w{1,2}$'
        ]
        for pattern in reject_patterns:
            if re.match(pattern, text.lower()):
                return False
        return True

    def calculate_heading_confidence(self, block: Dict, text: str) -> float:
        confidence = 0
        if block['is_bold'] or block['all_bold']:
            confidence += 20
        if re.match(r'^\d+\.?\s+[A-Z]', text):
            confidence += 25
        elif re.match(r'^\d+\.\d+\s+[A-Z]', text):
            confidence += 20
        elif re.match(r'^\d+\.\d+\.\d+\s+[A-Z]', text):
            confidence += 15
        heading_words = [
            'introduction', 'conclusion', 'abstract', 'methodology', 'results',
            'discussion', 'background', 'literature', 'review', 'chapter',
            'section', 'summary', 'overview', 'analysis', 'findings'
        ]
        if any(word in text.lower() for word in heading_words):
            confidence += 15
        if text.isupper() and len(text) > 8:
            confidence += 10
        if text.istitle() and len(text.split()) >= 2:
            confidence += 10
        if block['x'] < 100:
            confidence += 5
        return confidence

    def validate_heading_sequence(self, headings: List[Dict]) -> List[Dict]:
        if not headings:
            return []
        validated = []
        for heading in headings:
            text = heading['text']
            valid = True
            words = text.split()
            if len(words) == 1 and len(text) < 15:
                single_word_patterns = [
                    r'^(introduction|conclusion|abstract|methodology|results|discussion|background|summary|overview|preface|glossary|references|bibliography|appendix)$'
                ]
                if not any(re.match(pattern, text.lower()) for pattern in single_word_patterns):
                    valid = False
            suspicious_words = ['young', 'rats', 'aged', 'rest', 'long', 'short', 'stimulation']
            if text.lower() in suspicious_words:
                valid = False
            if not (text[0].isupper() or text[0].isdigit() or text.isupper()):
                valid = False
            if valid:
                validated.append({
                    "level": heading["level"],
                    "text": heading["text"],
                    "page": heading["page"]
                })
        return validated

    def process_single_pdf_universal(self, pdf_path: str) -> Tuple[Dict, Dict]:
        start_time = datetime.now()
        filename = os.path.basename(pdf_path)
        print(f"\nüöÄ  PDF Processing: {filename}")
        print("-" * 60)
        text_blocks, full_text = self.extract_text_with_fonts(pdf_path)
        font_analysis = self.analyze_font_structure(text_blocks)
        title = self.detect_title_universal(text_blocks, font_analysis)
        headings = self.detect_headings_hierarchical(text_blocks, font_analysis)
        processing_time = (datetime.now() - start_time).total_seconds()
        adobe_result = {
            "title": title,
            "outline": headings
        }
        enhanced_result = {
            "title": title,
            "outline": headings,
            "font_analysis": {
                "body_font_size": font_analysis['body_font_size'],
                "font_range": f"{font_analysis['font_stats']['min_size']}-{font_analysis['font_stats']['max_size']}pt",
                "heading_levels": len(font_analysis['heading_hierarchy']),
                "title_candidates_count": len(font_analysis['title_candidates']),
                "consolidation_improved": True
            },
            "processing_info": {
                "filename": filename,
                "processing_time_seconds": round(processing_time, 2),
                "total_headings_found": len(headings),
                "document_pages": max([b['page'] for b in text_blocks]) if text_blocks else 0,
                "extraction_method": "Universal Hierarchical Analysis with Smart Consolidation"
            }
        }
        print(f"‚úÖ Processing complete in {processing_time:.2f} seconds")
        print(f"üìä Results: Title + {len(headings)} complete headings")
        return adobe_result, enhanced_result

def setup_environment():
    drive.mount('/content/drive')
    project_dir = '/content/drive/MyDrive/Adobe_Hackathon_Universal'
    cache_dir = f'{project_dir}/cache'
    os.makedirs(project_dir, exist_ok=True)
    os.makedirs(cache_dir, exist_ok=True)
    print("‚úÖ Environment setup complete!")
    return project_dir, cache_dir

def run_universal_analysis(cache_dir):
    import glob

    print("üéØ Universal PDF Structure Extractor - Complete Single Headings")
    print("=" * 70)
    print("üì§ Loading PDF files from 'pdfs/' folder...")

    # Load all PDFs from the local 'pdfs/' folder
    pdf_paths = glob.glob("pdfs/*.pdf")
    if not pdf_paths:
        print("‚ùå No PDF files found in 'pdfs/' folder.")
        return

    processor = UniversalPDFExtractor(cache_dir)

    for pdf_path in pdf_paths:
        filename = os.path.basename(pdf_path)
        print(f"\nüìÑ Processing: {filename}")

        try:
            adobe_result, enhanced_result = processor.process_single_pdf_universal(pdf_path)
            display_universal_results(enhanced_result, filename)
            save_universal_results(adobe_result, enhanced_result, filename)
        except Exception as e:
            print(f"‚ùå Error processing {filename}: {e}")
            import traceback
            traceback.print_exc()

    print("\n‚úÖ All PDFs processed successfully.")



def display_universal_results(result: Dict, filename: str):
    font_info = result['font_analysis']
    processing_info = result['processing_info']
    html_output = f"""
    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 25px; border-radius: 15px; color: white; font-family: Arial, sans-serif;'>
        <h2 style='margin-top: 0; color: #fff; text-align: center;'>üéØ PDF Analysis: {filename}</h2>
        <div style='background: rgba(255,255,255,0.15); padding: 15px; border-radius: 10px; margin: 15px 0;'>
            <h3 style='color: #ffd700; margin-top: 0;'>üìù Document Title</h3>
            <p style='font-size: 18px; font-weight: bold;'>{result['title']}</p>
        </div>
        <div style='background: rgba(255,255,255,0.15); padding: 15px; border-radius: 10px; margin: 15px 0;'>
            <h3 style='color: #98fb98; margin-top: 0;'>üìä Font Analysis</h3>
            <div style='display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px;'>
                <div style='background: rgba(255,255,255,0.1); padding: 10px; border-radius: 5px;'>
                    <strong>Body Font:</strong> {font_info['body_font_size']}pt
                </div>
                <div style='background: rgba(255,255,255,0.1); padding: 10px; border-radius: 5px;'>
                    <strong>Font Range:</strong> {font_info['font_range']}
                </div>
                <div style='background: rgba(255,255,255,0.1); padding: 10px; border-radius: 5px;'>
                    <strong>Heading Levels:</strong> {font_info['heading_levels']}
                </div>
                <div style='background: rgba(255,255,255,0.1); padding: 10px; border-radius: 5px;'>
                    <strong>‚úÖ Smart Consolidation:</strong> Enabled
                </div>
            </div>
        </div>
        <div style='background: rgba(255,255,255,0.15); padding: 15px; border-radius: 10px; margin: 15px 0;'>
            <h3 style='color: #87ceeb; margin-top: 0;'>üìã Complete Document Structure ({len(result['outline'])} headings)</h3>
            <ul style='list-style: none; padding-left: 0;'>
    """
    level_colors = {'H1': '#ff6b6b', 'H2': '#4ecdc4', 'H3': '#45b7d1'}
    level_icons = {'H1': 'üî¥', 'H2': 'üîµ', 'H3': 'üü¢'}
    for heading in result['outline']:
        color = level_colors.get(heading['level'], '#ffffff')
        icon = level_icons.get(heading['level'], '‚ö™')
        indent = '&nbsp;' * (int(heading['level'][1]) - 1) * 6
        html_output += f"""
        <li style='margin: 8px 0; padding: 12px; background: rgba(255,255,255,0.1); border-radius: 8px;'>
            {indent}{icon} <span style='color: {color}; font-weight: bold;'>{heading['level']}</span>:
            <span style='color: #fff; font-size: 16px;'>{heading['text']}</span>
            <span style='color: #ffd700; font-size: 12px; float: right;'>Page {heading['page']}</span>
        </li>
        """
    html_output += f"""
            </ul>
        </div>
        <div style='background: rgba(255,255,255,0.1); padding: 10px; border-radius: 8px; font-size: 14px; text-align: center;'>
            ‚è±Ô∏è {processing_info['processing_time_seconds']}s | üìÑ {processing_info['document_pages']} pages |
            üéØ {processing_info['total_headings_found']} complete headings | üîß {processing_info['extraction_method']}
        </div>
    </div>
    """
    print("üìã Summary Output:")
    print(html_output)  # Or print formatted summaries as text



def save_universal_results(adobe_result: Dict, enhanced_result: Dict, filename: str):
    import os

    base_name = filename.replace('.pdf', '')
    output_dir = "outputs"
    os.makedirs(output_dir, exist_ok=True)

    adobe_filename = os.path.join(output_dir, f"{base_name}_adobe_consolidated.json")

    with open(adobe_filename, 'w', encoding='utf-8') as f:
        json.dump(adobe_result, f, indent=2, ensure_ascii=False)

    print(f"\nüíæ Results saved to: {adobe_filename}")
    print(f"\nüìã Adobe Hackathon JSON Output (Preview):")
    print("```")
    print(json.dumps(adobe_result, indent=2, ensure_ascii=False)[:2000] + "...\n```")  # Print only first 2000 chars


if __name__ == "__main__":
    project_dir = os.getcwd()
    cache_dir = os.path.join(project_dir, "cache")
    os.makedirs(cache_dir, exist_ok=True)
    run_universal_analysis(cache_dir)
